data_cfg:
  dataset:
    processor:
      feat_extractor_id: ${model_cfg.model.encoder_id}
      tokenizer_id: ${model_cfg.tokenizer_id}
    path:
      base:
        indict_tts: ../IndicTTS
        cv: ../
      train:
        - train_data/indict_tts_train.jsonl
        - train_data/cv_train.jsonl
      test:
        - train_data/indict_tts_test.jsonl
        - train_data/cv_test.jsonl
      dev:
        - train_data/indict_tts_dev.jsonl
        - train_data/cv_dev.jsonl
  dataloader:
    batch_size: 64
    num_workers: 8
    pin_memory: True

model_cfg:
  tokenizer_id: tuanio/wav2vec2-phoneme-ipa-ctc
  model:
    encoder_id: tuanio/whisper-encoder.tiny.en
  optim:
    lr: 3e-4
    betas: [0.9, 0.95]
    weight_decay: 0.01
  scheduler:
    name: linear
    total_steps: -1
    warmup_ratio: 0.05
    interval: step
    frequency: 1

trainer_cfg:
  log:
    wandb: True
  logger_wandb:
    project: aped_indian-lish
    name: whisper-tiny-1
    log_model: all
  callbacks:
    early_stop:
      monitor: valid/loss
      min_delta: 0.0
      patience: 100
      verbose: True
      mode: min
      strict: True
      check_finite: True
      stopping_threshold: null
      divergence_threshold: null
      check_on_train_epoch_end: null
      log_rank_zero_only: False
  arguments:
    accelerator: gpu
    devices: -1
    max_epochs: 10
    log_every_n_steps: 1
    enable_checkpointing: True
    accumulate_grad_batches: 2
    inference_mode: True
    gradient_clip_val: 2.0


experiment_cfg:
  train: True
  test: True
  ckpt:
    resume_ckpt: False
    ckpt_path: ...